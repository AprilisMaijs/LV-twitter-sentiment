{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yA8kElBnkSmU",
        "QEfM_47glx2P",
        "RqoC1wBpqsMZ",
        "8BmW4bkzmNb4",
        "oEnYand-kAdm"
      ],
      "private_outputs": true,
      "toc_visible": true,
      "mount_file_id": "1idrlsdrGocDWYjV13CYEf_Vu32BrKI3S",
      "authorship_tag": "ABX9TyPjVtc3eJiZTszfIjmlltY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AprilisMaijs/bakalaurs-twitter-sentiment/blob/neuralnet_training/LVTwitSentimenti.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and importing dependencies"
      ],
      "metadata": {
        "id": "yA8kElBnkSmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n"
      ],
      "metadata": {
        "id": "cD4EM2n0jpMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers snscrape beautifulsoup4 pandas numpy"
      ],
      "metadata": {
        "id": "YMzylZWUku7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "T3keqZChlSo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "gKn-kb9mXt5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "PSlzAtehZEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training data preprocessing\n"
      ],
      "metadata": {
        "id": "HX1VnEfdrROQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/bakalaurs/marked_train.txt', delimiter='|',encoding = 'UTF-8')\n",
        "df_eval = pd.read_csv('/content/drive/MyDrive/bakalaurs/marked_eval.txt', delimiter='|',encoding = 'UTF-8')\n",
        "df_train.columns = ['LABEL_COLUMN', 'DATA_COLUMN']\n",
        "df_eval.columns = ['LABEL_COLUMN', 'DATA_COLUMN']"
      ],
      "metadata": {
        "id": "MI17VcsA1Yx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_train)\n",
        "display(df_eval)"
      ],
      "metadata": {
        "id": "BZUOV135zqaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.sort_values(by=['LABEL_COLUMN'])"
      ],
      "metadata": {
        "id": "rIOObsv77HHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.sample(10)"
      ],
      "metadata": {
        "id": "YnVMdRul8xv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regexes - Alīna Govoruhina(2022)\n",
        "newLine =\"\\\\n|\\\\r\"\n",
        "urls = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
        "numbers = '\\d+((\\.|\\-)\\d+)?'\n",
        "mentions = '\\B\\@([\\w\\-]+)'\n",
        "hashtag = '#'\n",
        "whitespaces = '\\s+'\n",
        "leadTrailWhitespace = '^\\s+|\\s+?$'\n",
        "\n",
        "\n",
        "#combining regexes into single \n",
        "combined_pattern = f'{newLine}|{urls}|{numbers}|{mentions}|{hashtag}'\n",
        "df_train['DATA_COLUMN'] = df_train['DATA_COLUMN'].str.replace(combined_pattern, '')\n",
        "df_eval['DATA_COLUMN'] = df_eval['DATA_COLUMN'].str.replace(combined_pattern, '')\n"
      ],
      "metadata": {
        "id": "yrJqjqy487L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['DATA_COLUMN', 'LABEL_COLUMN']]\n",
        "df_eval = df_eval[['DATA_COLUMN', 'LABEL_COLUMN']]"
      ],
      "metadata": {
        "id": "ud6myq1-HX6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.sample(10)"
      ],
      "metadata": {
        "id": "MZErtc4oHjix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face fine-tuning guide\n"
      ],
      "metadata": {
        "id": "eQ2m5-ydUJS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"AiLab-IMCS-UL/lvbert\", num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"AiLab-IMCS-UL/lvbert\")"
      ],
      "metadata": {
        "id": "8DRpCZzsUNNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "ds_train = Dataset.from_pandas(df_train)\n",
        "ds_test = Dataset.from_pandas(df_eval)"
      ],
      "metadata": {
        "id": "hN0t-P6BVO7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train[\"DATA_COLUMN\"][:10]"
      ],
      "metadata": {
        "id": "rxBkHfEHaF5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"AiLab-IMCS-UL/lvbert\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"DATA_COLUMN\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds_train.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "qmfTrwJhWMcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")"
      ],
      "metadata": {
        "id": "3lvzTmyFYY3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "qGppbhgFZA5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "jsM7fpRCZNZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
      ],
      "metadata": {
        "id": "AZlDWg8WZTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "vWSzGHAIZWlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "qDzJnf59Zgo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate LVBERT model 'AiLab-IMCS-UL/lvbert'\n",
        "\n"
      ],
      "metadata": {
        "id": "QEfM_47glx2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"AiLab-IMCS-UL/lvbert\", from_pt=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"AiLab-IMCS-UL/lvbert\")"
      ],
      "metadata": {
        "id": "F6pzrm5HCw_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "njozFOMeEx3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 — convert_data_to_examples: This will accept our train and test datasets and convert each row into an InputExample object.\n",
        "\n",
        "2 — convert_examples_to_tf_dataset: This function will tokenize the InputExample objects, then create the required input format with the tokenized objects, finally, create an input dataset that we can feed to the model.\n"
      ],
      "metadata": {
        "id": "pgsz_bYAHzZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
        "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
        "  \n",
        "  return train_InputExamples, validation_InputExamples\n",
        "\n",
        "  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n",
        "                                                                           test, \n",
        "                                                                           'DATA_COLUMN', \n",
        "                                                                           'LABEL_COLUMN')\n",
        "  \n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] # -> will hold InputFeatures to be converted later\n",
        "\n",
        "    for e in examples:\n",
        "        # Documentation is really strong for this method, so please take a look at it\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, # truncates if len(s) > max_length\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "DATA_COLUMN = 'DATA_COLUMN'\n",
        "LABEL_COLUMN = 'LABEL_COLUMN'"
      ],
      "metadata": {
        "id": "af-i5YS_FNe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_train\n",
        "test = df_eval"
      ],
      "metadata": {
        "id": "FEXjL0fUOpEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.sample(10)"
      ],
      "metadata": {
        "id": "0omu4iwTOwKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_InputExamples, validation_InputExamples = convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN)\n",
        "\n",
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)"
      ],
      "metadata": {
        "id": "ycTNkLzfHyZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sentences = ['Iespējams sliktākā partija pasaulē - nekādas atbidības, naudas izšķērdēšana un korupcija valda visu!','Paldies partijai par jaunajām ietvēm! Manam kaķim un man arī ļoti patīk!']"
      ],
      "metadata": {
        "id": "OBffK3IwIs7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "model.fit(train_data, epochs=2, validation_data=validation_data)"
      ],
      "metadata": {
        "id": "TuQQjfglJLVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoKLlRaCWGyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate mBERT model\n"
      ],
      "metadata": {
        "id": "RqoC1wBpqsMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode('Man riebās. Pretīga izrāde un nepatīkami aktieri.', return_tensors='pt')"
      ],
      "metadata": {
        "id": "IlFsRL8FmRyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(tokens)"
      ],
      "metadata": {
        "id": "kosbmNfrmcwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.logits"
      ],
      "metadata": {
        "id": "TEk4XXISmMgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int(torch.argmax(result.logits))"
      ],
      "metadata": {
        "id": "zvyxfsQfmzan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting tweets WITHOUT API\n",
        "\n"
      ],
      "metadata": {
        "id": "oEnYand-kAdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from datetime import date"
      ],
      "metadata": {
        "id": "aEZeiaL7Re-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tweet_data = []\n",
        "\n",
        "#query = input('enter your keyword: ')\n",
        "query = 'vienotība since:2022-11-28 until:2022-12-31'\n",
        "number = int(input('how many tweets to scrape: '))\n",
        "for i, tweets in enumerate(sntwitter.TwitterSearchScraper('{}'.format(query)).get_items()):\n",
        "    if i > number:\n",
        "        break\n",
        "    tweet_data.append([tweets.date, tweets.content, tweets.username, tweets.url])\n",
        "\n",
        "df = pd.DataFrame(tweet_data, columns = ['Date', 'Tweets', 'Username', 'URL'])\n",
        "df.to_csv(f'{query}.csv', index=False, encoding='utf-8')\n",
        "\n"
      ],
      "metadata": {
        "id": "3CtW1F-NRuep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}